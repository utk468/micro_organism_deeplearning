{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utk468/micro_organism_deeplearning/blob/main/micro_organism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import shutil\n",
        "from shutil import copytree, rmtree\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "d3hRfGhaQPJP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(source_dir, target_dir, split_ratio=0.8):\n",
        "\n",
        "    if os.path.exists(target_dir):\n",
        "        rmtree(target_dir)\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "    classes = os.listdir(source_dir)\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(source_dir, cls)\n",
        "        if os.path.isdir(cls_path):\n",
        "            images = os.listdir(cls_path)\n",
        "            train_size = int(len(images) * split_ratio)\n",
        "\n",
        "            os.makedirs(os.path.join(target_dir, 'train', cls))\n",
        "            os.makedirs(os.path.join(target_dir, 'validation', cls))\n",
        "\n",
        "            for i, img in enumerate(images):\n",
        "                src_path = os.path.join(cls_path, img)\n",
        "                if os.path.isfile(src_path):\n",
        "                    if i < train_size:\n",
        "                        dest_path = os.path.join(target_dir, 'train', cls, img)\n",
        "                    else:\n",
        "                        dest_path = os.path.join(target_dir, 'validation', cls, img)\n",
        "                    shutil.copy(src_path, dest_path)\n",
        "\n",
        "source_dir = \"/content/drive/MyDrive/Colab Notebooks/Micro_Organism\"\n",
        "target_dir = \"organized_dataset\"\n",
        "prepare_data(source_dir, target_dir)"
      ],
      "metadata": {
        "id": "8CtEu_LoQW8H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oHiZnuPv14eq"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dir = os.path.join(target_dir, 'train')\n",
        "validation_dir = os.path.join(target_dir, 'validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irVvdMMm1_V6",
        "outputId": "5baf3b46-8a74-489c-8878-45a9b6aff638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 636 images belonging to 8 classes.\n",
            "Found 163 images belonging to 8 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\"\n",
        ")\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(train_generator.class_indices), activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "\n",
        "accuracy = model.evaluate(validation_generator)[1] * 100\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "lQSmwmu6odrc",
        "outputId": "153a58ae-4a21-4804-ada8-ef8173ab4749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.1395 - loss: 2.7193 - val_accuracy: 0.1963 - val_loss: 2.0216\n",
            "Epoch 2/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - accuracy: 0.2047 - loss: 2.2832 - val_accuracy: 0.2699 - val_loss: 1.8623\n",
            "Epoch 3/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - accuracy: 0.2362 - loss: 2.1221 - val_accuracy: 0.3558 - val_loss: 1.7639\n",
            "Epoch 4/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.3075 - loss: 1.8869 - val_accuracy: 0.3865 - val_loss: 1.6818\n",
            "Epoch 5/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - accuracy: 0.3405 - loss: 1.8093 - val_accuracy: 0.4110 - val_loss: 1.6131\n",
            "Epoch 6/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - accuracy: 0.3848 - loss: 1.7231 - val_accuracy: 0.4479 - val_loss: 1.5531\n",
            "Epoch 7/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.3803 - loss: 1.6888 - val_accuracy: 0.4908 - val_loss: 1.5036\n",
            "Epoch 8/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.4334 - loss: 1.6035 - val_accuracy: 0.4908 - val_loss: 1.4594\n",
            "Epoch 9/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.4709 - loss: 1.5583 - val_accuracy: 0.5031 - val_loss: 1.4141\n",
            "Epoch 10/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.4622 - loss: 1.5098 - val_accuracy: 0.5153 - val_loss: 1.3803\n",
            "Epoch 11/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.5148 - loss: 1.4082 - val_accuracy: 0.5399 - val_loss: 1.3410\n",
            "Epoch 12/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.5068 - loss: 1.3986 - val_accuracy: 0.5460 - val_loss: 1.3115\n",
            "Epoch 13/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - accuracy: 0.4914 - loss: 1.3460 - val_accuracy: 0.5890 - val_loss: 1.2685\n",
            "Epoch 14/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.5616 - loss: 1.2809 - val_accuracy: 0.6074 - val_loss: 1.2437\n",
            "Epoch 15/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.5720 - loss: 1.2538 - val_accuracy: 0.5828 - val_loss: 1.2290\n",
            "Epoch 16/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.5618 - loss: 1.2686 - val_accuracy: 0.6196 - val_loss: 1.2056\n",
            "Epoch 17/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.6086 - loss: 1.2169 - val_accuracy: 0.6196 - val_loss: 1.1757\n",
            "Epoch 18/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - accuracy: 0.6260 - loss: 1.1787 - val_accuracy: 0.6135 - val_loss: 1.1575\n",
            "Epoch 19/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.6017 - loss: 1.1374 - val_accuracy: 0.6258 - val_loss: 1.1578\n",
            "Epoch 20/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.6381 - loss: 1.0612 - val_accuracy: 0.6196 - val_loss: 1.1465\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6423 - loss: 1.1051\n",
            "Validation Accuracy: 61.96%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KDMVtyuuHlEwZPyE2_pY70u13Vl_cVIx",
      "authorship_tag": "ABX9TyOfvdv+db08EVjeRjcIm4ik",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}